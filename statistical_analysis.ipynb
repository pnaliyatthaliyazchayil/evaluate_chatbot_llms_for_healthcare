{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d9534a-22fb-411e-90f7-2fc2806a11ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model_1  Model_2  Wilcoxon_stat   p_value  p_value_bonferroni  Significant\n",
      "0    llama  chatgpt            0.0  0.250000            2.500000        False\n",
      "1    llama   gemini            0.0  0.250000            2.500000        False\n",
      "2    llama       o3            0.0  0.250000            2.500000        False\n",
      "4  chatgpt   gemini            0.0  0.250000            2.500000        False\n",
      "5  chatgpt       o3            0.0  0.250000            2.500000        False\n",
      "7   gemini       o3            0.0  0.250000            2.500000        False\n",
      "8   gemini       R1            0.0  0.250000            2.500000        False\n",
      "9       o3       R1            1.0  0.500000            5.000000        False\n",
      "3    llama       R1            1.0  0.654721            6.547208        False\n",
      "6  chatgpt       R1            2.0  0.750000            7.500000        False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parvati\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4088: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "C:\\Users\\Parvati\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_morestats.py:4102: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "#added by parvati \n",
    "#wilcoxsons\n",
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "from itertools import combinations\n",
    "\n",
    "# Your data\n",
    "data = {\n",
    "    \"task_name\": [\"primary_diagnaois\", \"ICD9_code\", \"readmission_risk\"],\n",
    "    \"llama\": [85, 42.67, 41.33],\n",
    "    \"chatgpt\": [84.9, 40.67, 40.67],\n",
    "    \"gemini\": [79, 14.67, 33],\n",
    "    \"o3\": [90, 45.33, 70.7],\n",
    "    \"R1\": [85, 40.33, 72.67]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index(\"task_name\", inplace=True)\n",
    "\n",
    "# Wilcoxon pairwise\n",
    "results = []\n",
    "for m1, m2 in combinations(df.columns, 2):\n",
    "    try:\n",
    "        stat, p = wilcoxon(df[m1], df[m2])\n",
    "        results.append({\n",
    "            \"Model_1\": m1,\n",
    "            \"Model_2\": m2,\n",
    "            \"Wilcoxon_stat\": stat,\n",
    "            \"p_value\": p\n",
    "        })\n",
    "    except ValueError as e:\n",
    "        results.append({\n",
    "            \"Model_1\": m1,\n",
    "            \"Model_2\": m2,\n",
    "            \"Wilcoxon_stat\": None,\n",
    "            \"p_value\": None,\n",
    "            \"Error\": str(e)\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df[\"p_value_bonferroni\"] = results_df[\"p_value\"] * len(results_df)\n",
    "results_df[\"Significant\"] = results_df[\"p_value_bonferroni\"] < 0.05\n",
    "\n",
    "print(results_df.sort_values(\"p_value_bonferroni\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defe9d8b-338d-44c2-9e6b-f80603ec5334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model  Mean_Accuracy  95% CI Lower  95% CI Upper\n",
      "3       o3          68.68         45.33          90.0\n",
      "4       R1          66.00         40.33          85.0\n",
      "0    llama          56.33         41.33          85.0\n",
      "1  chatgpt          55.41         40.67          84.9\n",
      "2   gemini          42.22         14.67          79.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your accuracy table (converted to decimal form)\n",
    "data = {\n",
    "    \"task_name\": [\"primary_diagnaois\", \"ICD9_code\", \"readmission_risk\"],\n",
    "    \"llama\": [85, 42.67, 41.33],\n",
    "    \"chatgpt\": [84.9, 40.67, 40.67],\n",
    "    \"gemini\": [79, 14.67, 33],\n",
    "    \"o3\": [90, 45.33, 70.7],\n",
    "    \"R1\": [85, 40.33, 72.67]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index(\"task_name\", inplace=True)\n",
    "\n",
    "# Bootstrap function\n",
    "def bootstrap_ci(data, n_boot=10000, ci=95):\n",
    "    means = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = data.sample(frac=1, replace=True)\n",
    "        means.append(sample.mean())\n",
    "    lower = np.percentile(means, (100 - ci) / 2)\n",
    "    upper = np.percentile(means, 100 - (100 - ci) / 2)\n",
    "    return data.mean(), lower, upper\n",
    "\n",
    "# Apply bootstrap for each model\n",
    "results = []\n",
    "for col in df.columns:\n",
    "    mean, low, high = bootstrap_ci(df[col])\n",
    "    results.append({\n",
    "        \"Model\": col,\n",
    "        \"Mean_Accuracy\": round(mean, 2),\n",
    "        \"95% CI Lower\": round(low, 2),\n",
    "        \"95% CI Upper\": round(high, 2)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(\"Mean_Accuracy\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64210bb5-b912-4b16-9a6c-c31c86fa588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Model_1  Model_2  U_stat   p_value  p_value_bonferroni  Significant\n",
      "0    llama  chatgpt     7.0  0.375825            3.758251        False\n",
      "4  chatgpt   gemini     7.0  0.375825            3.758251        False\n",
      "5  chatgpt       o3     2.0  0.375825            3.758251        False\n",
      "1    llama   gemini     7.0  0.400000            4.000000        False\n",
      "2    llama       o3     2.0  0.400000            4.000000        False\n",
      "7   gemini       o3     2.0  0.400000            4.000000        False\n",
      "8   gemini       R1     2.0  0.400000            4.000000        False\n",
      "3    llama       R1     4.5  1.000000           10.000000        False\n",
      "6  chatgpt       R1     4.0  1.000000           10.000000        False\n",
      "9       o3       R1     5.0  1.000000           10.000000        False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import mannwhitneyu\n",
    "from itertools import combinations\n",
    "\n",
    "# Your accuracy data\n",
    "data = {\n",
    "    \"task_name\": [\"primary_diagnaois\", \"ICD9_code\", \"readmission_risk\"],\n",
    "    \"llama\": [85, 42.67, 41.33],\n",
    "    \"chatgpt\": [84.9, 40.67, 40.67],\n",
    "    \"gemini\": [79, 14.67, 33],\n",
    "    \"o3\": [90, 45.33, 70.7],\n",
    "    \"R1\": [85, 40.33, 72.67]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.set_index(\"task_name\", inplace=True)\n",
    "\n",
    "# Mann-Whitney U test pairwise\n",
    "results = []\n",
    "for m1, m2 in combinations(df.columns, 2):\n",
    "    stat, p = mannwhitneyu(df[m1], df[m2], alternative='two-sided')  # two-sided test\n",
    "    results.append({\n",
    "        \"Model_1\": m1,\n",
    "        \"Model_2\": m2,\n",
    "        \"U_stat\": stat,\n",
    "        \"p_value\": p\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df[\"p_value_bonferroni\"] = results_df[\"p_value\"] * len(results_df)\n",
    "results_df[\"Significant\"] = results_df[\"p_value_bonferroni\"] < 0.05\n",
    "\n",
    "print(results_df.sort_values(\"p_value_bonferroni\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba5241-9cc9-471b-8386-88ade893f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code we used to get f1 scores we used in paper - just for your infor if you need this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744bcb4-c78c-44db-9dfa-d21ebdfc47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('C:/Users/Parvati/Desktop/mimic/llama_final_with_diagnosis_readmisison_sick_comparison.csv')\n",
    "\n",
    "# Step 1: Split the comma-separated strings into lists\n",
    "df['icd9_codes'] = df['icd9_codes'].apply(lambda x: [i.strip() for i in str(x).split(',')])\n",
    "df['converted_icd_code'] = df['converted_icd_code'].apply(lambda x: [i.strip() for i in str(x).split(',')])\n",
    "\n",
    "# Step 2: Get all unique ICD codes in the ground truth (only use the true labels for comparison)\n",
    "all_labels = set().union(*df['converted_icd_code'])\n",
    "\n",
    "# Step 3: Create binary indicator matrices for ground truths and predictions\n",
    "label_to_index = {label: i for i, label in enumerate(all_labels)}\n",
    "y_true = np.zeros((len(df), len(all_labels)), dtype=int)\n",
    "y_pred = np.zeros((len(df), len(all_labels)), dtype=int)\n",
    "\n",
    "# Fill in y_true and y_pred matrices based on ground_truth_icd and llama_predicted_icd\n",
    "for i, (true_labels, pred_labels) in enumerate(zip(df['converted_icd_code'], df['icd9_codes'])):\n",
    "    for label in true_labels:\n",
    "        y_true[i, label_to_index[label]] = 1\n",
    "    for label in pred_labels:\n",
    "        # Only mark as predicted if the label is in the ground truth set (for comparison)\n",
    "        if label in label_to_index:\n",
    "            y_pred[i, label_to_index[label]] = 1\n",
    "\n",
    "# Step 4: Calculate multilabel confusion matrices\n",
    "conf_matrix = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "Diagnosis_list = []\n",
    "Precision_list = []\n",
    "Recall_list = []\n",
    "F1_list = []\n",
    "Support_list = []\n",
    "TTP, TTN, TFP, TFN = [], [], [], []\n",
    "\n",
    "# Step 5: Calculate per-class metrics\n",
    "for label, conf in zip(all_labels, conf_matrix):\n",
    "    TP = conf[1, 1]\n",
    "    TN = conf[0, 0]\n",
    "    FP = conf[0, 1]\n",
    "    FN = conf[1, 0]\n",
    "    TTP.append(TP)\n",
    "    TTN.append(TN)\n",
    "    TFP.append(FP)\n",
    "    TFN.append(FN)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    Precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    Recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    F1 = (2 * Precision * Recall / (Precision + Recall)) if (Precision + Recall) > 0 else 0\n",
    "\n",
    "    Diagnosis_list.append(label)\n",
    "    Precision_list.append(Precision)\n",
    "    Recall_list.append(Recall)\n",
    "    F1_list.append(F1)\n",
    "    Support_list.append(np.sum(y_true[:, label_to_index[label]]))\n",
    "\n",
    "# Step 6: Create a DataFrame for per-class metrics\n",
    "per_class_metrics = pd.DataFrame({\n",
    "    'Diagnosis': Diagnosis_list,\n",
    "    'Precision': Precision_list,\n",
    "    'Recall': Recall_list,\n",
    "    'F1 Score': F1_list,\n",
    "    'Support': Support_list\n",
    "})\n",
    "print(\"Per-Class Metrics:\")\n",
    "print(per_class_metrics)\n",
    "\n",
    "# Step 7: Calculate aggregate metrics\n",
    "Precision_list, Recall_list, F1_list, Support_list = [], [], [], []\n",
    "list_section = ['micro avg', 'macro avg', 'weighted avg']\n",
    "\n",
    "for section in list_section:\n",
    "    if section == 'micro avg':\n",
    "        TP = sum(TTP)\n",
    "        FP = sum(TFP)\n",
    "        FN = sum(TFN)\n",
    "        Precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        Recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        F1 = (2 * Precision * Recall / (Precision + Recall)) if (Precision + Recall) > 0 else 0\n",
    "        Support = np.sum(Support_list)\n",
    "\n",
    "    elif section == 'macro avg':\n",
    "        Precision = np.mean(Precision_list)\n",
    "        Recall = np.mean(Recall_list)\n",
    "        F1 = np.mean(F1_list)\n",
    "        Support = np.sum(Support_list)\n",
    "\n",
    "    elif section == 'weighted avg':\n",
    "        total_support = np.sum(Support_list)\n",
    "        if total_support > 0:\n",
    "            Precision = np.sum(np.array(Precision_list) * np.array(Support_list)) / total_support\n",
    "            Recall = np.sum(np.array(Recall_list) * np.array(Support_list)) / total_support\n",
    "            F1 = np.sum(np.array(F1_list) * np.array(Support_list)) / total_support\n",
    "        else:\n",
    "            Precision, Recall, F1 = 0, 0, 0  # Handle division by zero case\n",
    "        Support = np.sum(Support_list)\n",
    "\n",
    "    Precision_list.append(Precision)\n",
    "    Recall_list.append(Recall)\n",
    "    F1_list.append(F1)\n",
    "    Support_list.append(Support)\n",
    "\n",
    "# Step 8: Create a DataFrame for aggregate metrics\n",
    "aggregate_metrics = pd.DataFrame({\n",
    "    'Section': list_section,\n",
    "    'Precision': Precision_list,\n",
    "    'Recall': Recall_list,\n",
    "    'F1 Score': F1_list,\n",
    "    'Support': Support_list\n",
    "})\n",
    "print(\"Aggregate Metrics:\")\n",
    "print(aggregate_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40e660-f5be-42a3-aa75-3dee169ea174",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this code below is boot strap confidence interval calculation. You have to tweak it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e82f0-c418-4194-b7e8-9734be612503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your function to compute F1 from the df (no changes here)\n",
    "def compute_micro_f1_from_df(df, prediction_col='llama_predicted_icd'):\n",
    "    df['llama_predicted_icd'] = df[prediction_col].apply(lambda x: [i.strip() for i in str(x).split(',')])\n",
    "    df['ground_truth_icd'] = df['ground_truth_icd'].apply(lambda x: [i.strip() for i in str(x).split(',')])\n",
    "\n",
    "    all_labels = set().union(*df['ground_truth_icd'])\n",
    "    label_to_index = {label: i for i, label in enumerate(all_labels)}\n",
    "    y_true = np.zeros((len(df), len(all_labels)), dtype=int)\n",
    "    y_pred = np.zeros((len(df), len(all_labels)), dtype=int)\n",
    "\n",
    "    for i, (true_labels, pred_labels) in enumerate(zip(df['ground_truth_icd'], df['llama_predicted_icd'])):\n",
    "        for label in true_labels:\n",
    "            y_true[i, label_to_index[label]] = 1\n",
    "        for label in pred_labels:\n",
    "            if label in label_to_index:\n",
    "                y_pred[i, label_to_index[label]] = 1\n",
    "\n",
    "    conf_matrix = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    TTP, TFP, TFN = 0, 0, 0\n",
    "    for conf in conf_matrix:\n",
    "        TTP += conf[1, 1]\n",
    "        TFP += conf[0, 1]\n",
    "        TFN += conf[1, 0]\n",
    "\n",
    "    Precision = TTP / (TTP + TFP) if (TTP + TFP) > 0 else 0\n",
    "    Recall = TTP / (TTP + TFN) if (TTP + TFN) > 0 else 0\n",
    "    F1 = (2 * Precision * Recall / (Precision + Recall)) if (Precision + Recall) > 0 else 0\n",
    "\n",
    "    return F1\n",
    "\n",
    "\n",
    "# Function to generate the null distribution by shuffling predictions\n",
    "def null_distribution_f1(df, prediction_col='llama_predicted_icd', n_iterations=1000):\n",
    "    null_scores = []\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        shuffled_df = df.copy()\n",
    "        shuffled_df[prediction_col] = np.random.permutation(df[prediction_col].values)\n",
    "        f1 = compute_micro_f1_from_df(shuffled_df, prediction_col=prediction_col)\n",
    "        null_scores.append(f1)\n",
    "\n",
    "    return np.array(null_scores)\n",
    "\n",
    "\n",
    "# Function to perform bootstrap resampling to compute confidence interval (CI) for F1 score\n",
    "def bootstrap_f1(df, prediction_col='llama_predicted_icd', n_iterations=1000):\n",
    "    bootstrap_scores = []\n",
    "\n",
    "    # Resampling with replacement\n",
    "    for _ in range(n_iterations):\n",
    "        bootstrap_sample = df.sample(n=len(df), replace=True)  # Bootstrap sampling\n",
    "        f1 = compute_micro_f1_from_df(bootstrap_sample, prediction_col=prediction_col)\n",
    "        bootstrap_scores.append(f1)\n",
    "\n",
    "    return np.array(bootstrap_scores)\n",
    "\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('test_f1_score_for_llama.csv', sep='\\t')\n",
    "\n",
    "# Step 1: Compute observed F1 using your existing method\n",
    "observed_f1 = compute_micro_f1_from_df(df)\n",
    "\n",
    "# Step 2: Generate the null distribution by shuffling predictions and calculating F1\n",
    "null_f1s = null_distribution_f1(df)\n",
    "\n",
    "# Step 3: Generate the bootstrap distribution for F1 and calculate CI\n",
    "bootstrap_f1s = bootstrap_f1(df)\n",
    "\n",
    "# Calculate 95% CI for the F1 score\n",
    "ci_lower = np.percentile(bootstrap_f1s, 2.5)\n",
    "ci_upper = np.percentile(bootstrap_f1s, 97.5)\n",
    "\n",
    "# Step 4: Calculate the p-value (How often are null F1s greater than or equal to observed F1)\n",
    "p_value = np.mean(null_f1s >= observed_f1)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Observed F1: {observed_f1}\")\n",
    "print(f\"Bootstrap 95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "print(f\"p-value (H0: F1 <= random): {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4954f96-13b1-417f-ad85-87c7571e62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is another version of the same that chatgpt gave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4c017-2874-494f-b212-a64ffc26902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define a function to calculate bootstrap CI for F1 score\n",
    "def bootstrap_f1_ci(df, n_iterations=1000, ci_percentile=95):\n",
    "    # Create a list to store F1 scores for each bootstrap iteration\n",
    "    bootstrap_f1_scores = []\n",
    "    \n",
    "    # Iterate for n_iterations to generate bootstrap samples\n",
    "    for _ in range(n_iterations):\n",
    "        # Resample the data with replacement\n",
    "        resample_data = df.sample(frac=1, replace=True)\n",
    "        \n",
    "        # Recalculate F1 score for the bootstrap sample\n",
    "        y_true = resample_data['ground_truth_icd']\n",
    "        y_pred = resample_data['llama_predicted_icd']\n",
    "        \n",
    "        # Convert lists of ICD codes into binary matrices for F1 calculation\n",
    "        all_labels = set().union(*y_true)\n",
    "        label_to_index = {label: i for i, label in enumerate(all_labels)}\n",
    "        \n",
    "        # Initialize matrices\n",
    "        y_true_matrix = np.zeros((len(resample_data), len(all_labels)), dtype=int)\n",
    "        y_pred_matrix = np.zeros((len(resample_data), len(all_labels)), dtype=int)\n",
    "        \n",
    "        for i, (true_labels, pred_labels) in enumerate(zip(y_true, y_pred)):\n",
    "            for label in true_labels:\n",
    "                y_true_matrix[i, label_to_index[label]] = 1\n",
    "            for label in pred_labels:\n",
    "                if label in label_to_index:\n",
    "                    y_pred_matrix[i, label_to_index[label]] = 1\n",
    "        \n",
    "        # Calculate F1 score for this resample\n",
    "        f1 = f1_score(y_true_matrix, y_pred_matrix, average='micro')\n",
    "        bootstrap_f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate the confidence intervals\n",
    "    lower_bound = np.percentile(bootstrap_f1_scores, (100 - ci_percentile) / 2)\n",
    "    upper_bound = np.percentile(bootstrap_f1_scores, 100 - (100 - ci_percentile) / 2)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Example usage\n",
    "lower_ci, upper_ci = bootstrap_f1_ci(df)\n",
    "\n",
    "# Output the Bootstrap Confidence Interval for the F1 score\n",
    "print(f\"Bootstrap 95% Confidence Interval for F1 Score: ({lower_ci:.4f}, {upper_ci:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
