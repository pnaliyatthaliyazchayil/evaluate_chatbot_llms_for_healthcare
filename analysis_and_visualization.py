# -*- coding: utf-8 -*-
"""
Created on Mon Nov 25 20:44:38 2024

@author: Parvati, Raajitha
"""
import pandas as pd
import re
import numpy as np

'''
This py script will show how the comparitive analysis and visualization was conducted for the study.

The same script can be applied on each LLMs response that has been extracted into csv.

The immediate below section demonstrates how ICD9 Match analysis was conducted between the LLM responses and the MIMIC-IV data
'''
#loading our file with all mimic data
df_final_with_icd = pd.read_csv('final_sample_300_with_icd_conversions.csv')

#loading the csv with LLM response into their respective columns. as example here we load the llama. we later also load the other two LLM responses here.
df_llm_responses =  pd.read_csv('llama_response_sectioned.csv')

# set the datatypes
df_llm_responses['icd9_codes'] = df_llm_responses['icd9_codes'].astype(str)
df_final_with_icd['converted_icd_code'] = df_final_with_icd['converted_icd_code'].astype(str)

df_llm_responses['subject_id'] = df_llm_responses['subject_id'].astype(str)
df_final_with_icd['subject_id'] = df_final_with_icd['subject_id'].astype(str)

# Function to clean ICD codes by removing periods and trailing spaces as MIMIC-IV data did not have periods
def clean_icd_codes(codes):
    if pd.isna(codes):
        return []
    # Remove periods, strip spaces, and split by comma
    return [code.strip().replace('.', '') for code in codes.split(',')]

#Clean 'icd9_codes' in df_gemini_responses
df_llm_responses['cleaned_icd9_codes'] = df_llm_responses['icd9_codes'].apply(clean_icd_codes)

#Clean 'converted_icd_code' in df_final_with_icd
df_final_with_icd['cleaned_converted_icd_code'] = df_final_with_icd['converted_icd_code'].apply(
    lambda codes: [code.strip() for code in codes.split(',')]
)

#Merge dataframes on 'subject_id' and 'hadm_id' to align rows
# This step ensures we're comparing corresponding rows
merged_df = pd.mergedf_llm_responses, df_final_with_icd[['subject_id', 'hadm_id', 'converted_icd_code', 'cleaned_converted_icd_code']], 
                     on=['subject_id', 'hadm_id'], 
                     how='left')

# Function to perform row-by-row matching
def row_match(row):
    icd9_list = set(row['cleaned_icd9_codes'])
    converted_codes = set(row['cleaned_converted_icd_code']) if isinstance(row['cleaned_converted_icd_code'], list) else set()
    
    # Find intersection for exact matches
    matched_codes = icd9_list.intersection(converted_codes)
    
    # Return matches as a comma-separated string if found, otherwise None
    return ', '.join(sorted(matched_codes)) if matched_codes else None

# Step 4: Apply the row_match function to each row in the merged dataframe
merged_df['icd_match_check'] = merged_df.apply(row_match, axis=1).astype(str)

# Optional: Drop the temporary cleaned columns, optional
merged_df.drop(columns=['cleaned_icd9_codes', 'cleaned_converted_icd_code'], inplace=True)

# Step 5: Select the relevant columns for the final output
# This includes all original columns from df_gemini_responses plus 'converted_icd_code' and 'icd_match_check'
final_df = merged_df.copy()

final_df.to_csv('llm_icd_match_check.csv', index=False)

'''
This section demonstrates how comparitive analysis was conducted between the Primary diagnosis generated by LLM responses and the MIMIC-IV data
'''

from sentence_transformers import SentenceTransformer, util

# Convert 'subject_id' and 'hadm_id' to string in both DataFrames for consistency
df_llm_responses['subject_id'] = df_llm_responses['subject_id'].astype(str)
df_llm_responses['hadm_id'] = df_llm_responses['hadm_id'].astype(str)

df_final_with_icd['subject_id'] = df_final_with_icd['subject_id'].astype(str)
df_final_with_icd['hadm_id'] = df_final_with_icd['hadm_id'].astype(str)

# Merge the DataFrames on 'subject_id' and 'hadm_id'
merged_df = pd.merge(
    df_llm_responses[['subject_id', 'hadm_id', 'primary_diagnosis']],
    df_final_with_icd[['subject_id', 'hadm_id', 'discharge_diagnosis']],
    on=['subject_id', 'hadm_id'],
    how='inner'
)

# Load a pre-trained model. SciBERT is specifically good for scientific and medical contexts.
model = SentenceTransformer('allenai/scibert_scivocab_uncased')

# Function to compare two diagnoses contextually using sentence embeddings
def are_diagnoses_similar(diagnosis1, diagnosis2, threshold=0.7):
    # Create embeddings for each diagnosis
    embedding1 = model.encode(diagnosis1, convert_to_tensor=True)
    embedding2 = model.encode(diagnosis2, convert_to_tensor=True)

    # Calculate cosine similarity between the two embeddings
    similarity = util.cos_sim(embedding1, embedding2).item()

    # Return True if similarity is above the threshold, otherwise False
    return similarity >= threshold

# Apply the comparison row by row and store the result in a new column 'diagnosis_match'
def compare_diagnoses(row, threshold=0.7):
    return are_diagnoses_similar(row['primary_diagnosis'], row['discharge_diagnosis'], threshold)

# Calculate diagnosis match and add the result to a new column 'diagnosis_match'
merged_df['diagnosis_match'] = merged_df.apply(compare_diagnoses, axis=1)

# Display the resulting DataFrame with the comparison
print(merged_df[['subject_id', 'hadm_id', 'primary_diagnosis', 'discharge_diagnosis', 'diagnosis_match']])

final_df['subject_id'] = final_df['subject_id'].astype(str)
final_df['hadm_id'] = final_df['hadm_id'].astype(str)

#merge with output of the icd code comparison
final_df = pd.merge(
    final_df, 
    merged_df[['subject_id', 'hadm_id', 'discharge_diagnosis', 'diagnosis_match']], 
    on=['subject_id', 'hadm_id'], 
    how='left'
)

# Display the resulting DataFrame to verify the merge
print(final_df[['subject_id', 'hadm_id', 'discharge_diagnosis', 'diagnosis_match']].head())

# Save it to a CSV file
output_file_path = 'llm_final_with_diagnosis_comparison.csv'
final_df.to_csv(output_file_path, index=False)

'''
This section demonstrates how comparitive analysis was conducted between the redamission risk generated by LLM responses and the MIMIC-IV data
'''

# Calculate quantiles (e.g., 25%, 50%, 75%)
quantiles = final_df['future_admission_count'].quantile([0.25, 0.5, 0.75])

# Print quantiles
print("Quantiles:\n", quantiles)

# Define bins based on quantiles
def categorize_admission_count(x):
    if x <= quantiles[0.25]:
        return 'Low'
    elif x <= quantiles[0.75]:
        return 'Medium'
    else:
        return 'High'

# Apply function to categorize the 'future_admission_count'
final_df['admission_category'] = final_df['future_admission_count'].apply(categorize_admission_count)

# Ensure consistency (remove trailing spaces and convert to lower case if needed)
final_df['readmission_risk'] = final_df['readmission_risk'].str.strip().str.lower()  # optional, if readmission_risk is string
final_df['admission_category'] = final_df['admission_category'].str.strip().str.lower()  # optional, if admission_category is string

#Compare the two columns and create a new column for the result (True if match, False if not)
final_df['readmission_match_check'] = final_df['readmission_risk'] == final_df['admission_category']

#Check the result
print(final_df[['subject_id', 'hadm_id', 'readmission_risk', 'admission_category', 'readmission_match_check']])

#Save it to a CSV file
output_file_path = 'llm_final_with_readmisison_comparison.csv'
final_df.to_csv(output_file_path, index=False)


'''
This section demonstrates how comparitive analysis was conducted between for less and more sick atients for LLM responses.
'''

#Count the number of ICD codes in the 'icd9_codes' column
# We count the commas and add 1 (as the number of ICD codes is one more than the number of commas)
final_df['num_icd_codes'] = final_df['icd9_codes'].str.count(',') + 1


from scipy import stats

#Calculate the z-scores of the 'num_icd_codes' column. The z-score standardizes the values based on the dataset's mean and standard deviation.if the z-score is greater than 0, it means the subject has an above-average number of ICD codes, and you categorize them as "more sick".
final_df['z_score'] = stats.zscore(final_df['num_icd_codes'])

#Categorize as "more sick" or "less sick" based on z-score
def categorize_sickness(z_score):
    if z_score > 0:
        return 'more sick'  # Above average
    else:
        return 'less sick'  # Average or below

# Apply the categorization function
final_df['sickness_category'] = final_df['z_score'].apply(categorize_sickness)

#Display the result
print(final_df[['subject_id', 'hadm_id', 'num_icd_codes', 'sickness_category']])

# Remove the 'z_score' column if it's not needed
final_df.drop('z_score', axis=1, inplace=True)

#Save it to a CSV file
output_file_path = 'llm_final_with_sick_comparison.csv'
final_df.to_csv(output_file_path, index=False)


'''
This section demonstrates how the top 10 ICD-9 codes from LLM responses and the MIMIC-IV data was generated
'''
from collections import Counter

#Split the 'icd9_codes' column into individual codes
# Remove any leading or trailing whitespace from the codes
df_final['icd9_codes'] = df_final['icd9_codes'].fillna('')  # Ensure no NaN values
all_codes = df_final['icd9_codes'].str.split(',').explode().str.strip()

#Count the occurrences of each individual ICD-9 code
code_counts = Counter(all_codes)

#Convert the counts to a DataFrame and sort by count
code_counts_df = pd.DataFrame(code_counts.items(), columns=['ICD9_Code', 'Count'])
code_counts_df = code_counts_df.sort_values(by='Count', ascending=False)

# Display the top ICD-9 codes with the highest counts
print(code_counts_df.head(10))  # Show the top 10 most frequent ICD-9 codes

#save the counts to a CSV if needed
#code_counts_df.to_csv('icd9_code_counts.csv', index=False)

#top icd9 codes of mimic 
# Split the 'icd9_codes' column into individual codes
# Remove any leading or trailing whitespace from the codes
df_final['converted_icd_code'] = df_final['converted_icd_code'].fillna('')  # Ensure no NaN values
all_codes_mimic = df_final['converted_icd_code'].str.split(',').explode().str.strip()

#Count the occurrences of each individual ICD-9 code
code_counts_mimic = Counter(all_codes_mimic)

#Convert the counts to a DataFrame and sort by count
code_counts_df_mimic = pd.DataFrame(code_counts_mimic.items(), columns=['ICD9_Code_mimic', 'Count_m'])
code_counts_df_mimic = code_counts_df_mimic.sort_values(by='Count_m', ascending=False)

# Display the top ICD-9 codes with the highest counts
print(code_counts_df_mimic.head(15))  # Show the top 10 most frequent ICD-9 codes

#save the counts to a CSV if needed
#code_counts_df.to_csv('icd9_code_counts_mimic.csv', index=False)


'''
This section demonstrates visualization of graphs generatd in study
'''

'''
This section demonstrates top 15 icd codes to graph for chatbot response
'''
import matplotlib.pyplot as plt

#visualize the top 10 ICD-9 codes with the highest counts
top_n = 15
top_codes_df = code_counts_df.head(top_n)

# Plotting the bar graph
plt.figure(figsize=(12, 8))
plt.barh(top_codes_df['ICD9_Code'], top_codes_df['Count'], color='skyblue')
plt.xlabel('Count')
plt.ylabel('ICD-9 Code')
plt.title(f'Top {top_n} Most Frequent ICD-9 Codes')
plt.gca().invert_yaxis()  # Invert the y-axis so the highest count is on top
plt.xticks(rotation=45)
plt.tight_layout()  # Adjust layout to make room for axis labels
plt.show()


'''
This section demonstrates top 15 icd codes to graph from MIMIC-V sample
'''
import matplotlib.pyplot as plt

#visualize the top 10 ICD-9 codes with the highest counts
top_n = 15
top_codes_df_mimic = code_counts_df_mimic.head(top_n)

# Plotting the bar graph
plt.figure(figsize=(12, 8))
plt.barh(top_codes_df_mimic['ICD9_Code_mimic'], top_codes_df_mimic['Count_m'], color='skyblue')
plt.xlabel('Count')
plt.ylabel('ICD-9 Code')
plt.title(f'Top {top_n} Most Frequent ICD-9 Codes in MIMIC-IV sample')
plt.gca().invert_yaxis()  # Invert the y-axis so the highest count is on top
plt.xticks(rotation=45)
plt.tight_layout()  # Adjust layout to make room for axis labels
plt.show()

'''
This section demonstrates graph for readmission risk
'''

import matplotlib.pyplot as plt

# Replace True/False with meaningful labels: "Correctly Predicted" / "Incorrectly Predicted"
readmission_counts = df_final_with_all_analysis['readmission_match_check'].replace({True: 'Correctly Predicted', False: 'Incorrectly Predicted'}).value_counts()

# Plot the pie chart with meaningful labels
plt.figure(figsize=(8, 8))
plt.pie(
    readmission_counts,
    labels=readmission_counts.index,  # Use "Correctly Predicted" / "Incorrectly Predicted" labels
    autopct='%1.1f%%',  # Display percentage
    startangle=140,     # Start angle for the first slice
    colors=['lightcoral', 'lightgreen'],  # Colors for Correctly and Incorrectly Predicted
    explode=(0.1, 0) if 'Correctly Predicted' in readmission_counts.index else (0, 0.1),  # Explode effect for Correctly Predicted
    textprops={'fontsize': 14}  # Increase font size of labels
)
plt.title('Readmission Prediction Comparison (MIMIC-IV vs. llama)', fontsize=16)
plt.axis('equal')  # Equal aspect ratio ensures pie chart is circular
plt.tight_layout()  # Adjust layout to fit title and labels
plt.show()

'''
This section demonstrates graph for primary diagnosis
'''

import matplotlib.pyplot as plt

# Replace True/False with meaningful labels: "Correctly Predicted" / "Incorrectly Predicted"
diagnosis_counts = df_final_with_all_analysis['diagnosis_match'].replace({True: 'Correctly Predicted', False: 'Incorrectly Predicted'}).value_counts()

# Plotting the donut chart
plt.figure(figsize=(8,10))

# Create the donut chart with a hole in the middle by setting the width of the wedges
plt.pie(diagnosis_counts, labels=diagnosis_counts.index, autopct='%1.1f%%', startangle=140, 
        colors=['lightgreen', 'lightcoral'], wedgeprops={'width': 0.4})  # This makes the donut

# Add title and adjust layout
plt.title('Primary Diagnosis Prediction Comparison (Donut Chart)', fontsize=20)

# Increase font size for labels and percentages on the slices
plt.setp(plt.gca().texts, fontsize=16)

# Equal aspect ratio ensures pie chart is drawn as a circle
plt.axis('equal')

# Show the plot
plt.tight_layout()
plt.show()

'''
This section demonstrates graph for correctly edicted ICD-9 codes between MIMIC-IV sample and LLM response
'''

# Count total subjects
total_subjects = df_final_with_all_analysis['subject_id'].nunique()


# Replace string 'None' with actual NaN
df_final_with_all_analysis['icd_match_check'] = df_final_with_all_analysis['icd_match_check'].replace('None', np.nan)


# Count rows where 'icd_match_check' is null or not null
correct_count = df_final_with_all_analysis['icd_match_check'].notna().sum()  # Non-null rows
incorrect_count = df_final_with_all_analysis['icd_match_check'].isna().sum()  # Null rows

# Prepare data for the bar graph
categories = ['Correct', 'Incorrect']
counts = [correct_count, incorrect_count]

# Plot the bar graph
plt.figure(figsize=(8, 6))
plt.bar(categories, counts, color=['green', 'red'], alpha=0.7)

# Add annotations
for i, count in enumerate(counts):
    plt.text(i, count + 5, f'{count}', ha='center', fontsize=12, fontweight='bold')

# Add titles and labels
plt.title(f'Correct vs. Incorrect ICD Match Checks (Total Subjects = {total_subjects})', fontsize=14)
plt.ylabel('Count', fontsize=12)
plt.xlabel('Match Status', fontsize=12)

# Display the plot
plt.show()

print(df_final_with_all_analysis['icd_match_check'].unique())


'''
This section demonstrates the F1 score calculation
'''
#icd9 codes f1 score; F1 Score: 0.0185 

def calculate_f1(row):
    # Split the comma-separated values into sets
    ground_truth = set(row['converted_icd_code'].split(",")) if pd.notna(row['converted_icd_code']) else set()
    predicted = set(row['icd9_codes'].split(",")) if pd.notna(row['icd9_codes']) else set()
    
    # Calculate True Positives, False Positives, and False Negatives
    tp = len(ground_truth & predicted)  # Intersection: True Positives
    fp = len(predicted - ground_truth)  # Predicted but not in ground truth: False Positives
    fn = len(ground_truth - predicted)  # Ground truth but not predicted: False Negatives
    
    # Calculate Precision and Recall
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    
    # Calculate F1 Score
    f1 = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0 else 0
    return f1

# Apply the F1 score calculation row-wise
df_final_with_all_analysis['f1_score'] = df_final_with_all_analysis.apply(calculate_f1, axis=1)

# Calculate the overall average F1 score
overall_f1 = df_final_with_all_analysis['f1_score'].mean()

# Print the results
print(df_final_with_all_analysis[['converted_icd_code', 'icd9_codes', 'f1_score']])
print(f"Overall F1 Score: {overall_f1}")


#readmission f1 score
#F1 Score for Readmission Prediction: F1 Score for low: 0.4516, F1 Score for medium: 0.3939, F1 Score for high: 0.3913, Macro-average F1 Score: 0.4123

from sklearn.metrics import precision_recall_fscore_support

# Calculate F1 score for categorical columns
def calculate_f1_categorical(row):
    # Extract ground truth and prediction
    ground_truth = row['admission_category']
    predicted = row['readmission_risk']
    
    # Return tuple of (ground truth, prediction) for calculating F1 score
    return ground_truth, predicted

# Apply the function across rows to extract ground truth and predictions
ground_truths, predictions = zip(*df_final_with_all_analysis.apply(calculate_f1_categorical, axis=1))

# Calculate precision, recall, and F1 score for each category (low, medium, high)
precision, recall, f1, _ = precision_recall_fscore_support(ground_truths, predictions, labels=['low', 'medium', 'high'], average=None)

# Display F1 score for each category
for category, f1_score in zip(['low', 'medium', 'high'], f1):
    print(f"F1 Score for {category}: {f1_score:.4f}")

# Optionally, calculate the macro-average F1 score
macro_f1 = precision_recall_fscore_support(ground_truths, predictions, labels=['low', 'medium', 'high'], average='macro')[2]
print(f"Macro-average F1 Score: {macro_f1:.4f}")

#-----------------------------------------------------these can be removed. these were for testing which script is correct--------------------------

from sklearn.metrics import f1_score

def calculate_f1_with_sklearn(row):
    # Split the  comma-separated values into sets
    ground_truth = set(map(str.strip, str(row['converted_icd_code']).split(","))) if pd.notna(row['converted_icd_code']) else set()
    predicted = set(map(str.strip, str(row['icd9_codes']).split(","))) if pd.notna(row['icd9_codes']) else set()

    
    # Union of all labels in the current row
    all_labels = sorted(ground_truth | predicted)
    
    # Binary indicator vectors for ground truth and predicted
    ground_truth_binary = [1 if label in ground_truth else 0 for label in all_labels]
    predicted_binary = [1 if label in predicted else 0 for label in all_labels]
    
    # Calculate F1 score for the row- Each ICD-9 code in a row is treated as a label, forming a multi-label scenario.
    #For weighted averaging, the F1 score for each label (tp, tn, fp, fn) is calculated separately under the hood and 
    #then combined using weights proportional to the number of true instances of each label in the ground truth.
    f1 = f1_score(ground_truth_binary, predicted_binary, average='weighted')  
    return f1

# Applying the F1 score calculation row-wise
df_final_with_all_analysis['f1_score'] = df_final_with_all_analysis.apply(calculate_f1_with_sklearn, axis=1)

# Calculating the overall weighted-average F1 score across all rows
overall_f1 = df_final_with_all_analysis['f1_score'].mean()

print(df_final_with_all_analysis[['converted_icd_code', 'icd9_codes', 'f1_score']])
df_results = df_final_with_all_analysis[['converted_icd_code', 'icd9_codes', 'f1_score']]

print(f"Overall Weighted-Average F1 Score: {overall_f1:.4f}")

from sklearn.metrics import f1_score

# Step 1: Extract all unique labels (ICD-9 codes) from the dataset
all_labels = sorted(
    set(
        icd.strip()
        for icd_list in df_final_with_all_analysis['converted_icd_code'].dropna().tolist() +
        df_final_with_all_analysis['icd9_codes'].dropna().tolist()
        for icd in str(icd_list).split(",")
    )
)

# Step 2: Define the function to calculate F1 score with the `labels` parameter
def calculate_f1_with_sklearn(row, labels):
    # Split the comma-separated values into sets
    ground_truth = set(map(str.strip, str(row['converted_icd_code']).split(","))) if pd.notna(row['converted_icd_code']) else set()
    predicted = set(map(str.strip, str(row['icd9_codes']).split(","))) if pd.notna(row['icd9_codes']) else set()

    # Union of all labels in the current row
    all_labels_row = sorted(ground_truth | predicted)
    
    # Binary indicator vectors for ground truth and predicted
    ground_truth_binary = [1 if label in ground_truth else 0 for label in labels]
    predicted_binary = [1 if label in predicted else 0 for label in labels]
    
    # Calculate F1 score for the rowall
    f1 = f1_score(ground_truth_binary, predicted_binary, average='weighted', labels=all_labels)
    return f1

# Step 3: Apply the F1 score calculation row-wise using the global set of labels
df_final_with_all_analysis['f1_score'] = df_final_with_all_analysis.apply(
    calculate_f1_with_sklearn,
    axis=1,
    labels=all_labels  # Pass the global set of labels
)

# Step 4: Calculate the overall weighted-average F1 score across all rows
overall_f1 = df_final_with_all_analysis['f1_score'].mean()

# Display the results
print(df_final_with_all_analysis[['converted_icd_code', 'icd9_codes', 'f1_score']])
print(f"Overall Weighted-Average F1 Score: {overall_f1:.4f}")













