# -*- coding: utf-8 -*-
"""
Created on Mon Nov 25 20:44:38 2024

@author: Parvati, Raajitha
"""
import pandas as pd
import re
import numpy as np

'''
This py script will show how the comparitive analysis and visualization was conducted for the study.

The same script can be applied on each LLMs response that has been extracted into csv.

The immediate below section demonstrates how ICD9 Match analysis was conducted between the LLM responses and the MIMIC-IV data
'''
#loading our file with all mimic data
df_final_with_icd = pd.read_csv('final_sample_300_with_icd_conversions.csv')

#loading the csv with LLM response into their respective columns. as example here we load the llama. we later also load the other two LLM responses here.
df_llm_responses =  pd.read_csv('llama_response_sectioned.csv')

# set the datatypes
df_llm_responses['icd9_codes'] = df_llm_responses['icd9_codes'].astype(str)
df_final_with_icd['converted_icd_code'] = df_final_with_icd['converted_icd_code'].astype(str)

df_llm_responses['subject_id'] = df_llm_responses['subject_id'].astype(str)
df_final_with_icd['subject_id'] = df_final_with_icd['subject_id'].astype(str)

# Function to clean ICD codes by removing periods and trailing spaces as MIMIC-IV data did not have periods
def clean_icd_codes(codes):
    if pd.isna(codes):
        return []
    # Remove periods, strip spaces, and split by comma
    return [code.strip().replace('.', '') for code in codes.split(',')]

#Clean 'icd9_codes' in df_gemini_responses
df_llm_responses['cleaned_icd9_codes'] = df_llm_responses['icd9_codes'].apply(clean_icd_codes)

#Clean 'converted_icd_code' in df_final_with_icd
df_final_with_icd['cleaned_converted_icd_code'] = df_final_with_icd['converted_icd_code'].apply(
    lambda codes: [code.strip() for code in codes.split(',')]
)

#Merge dataframes on 'subject_id' and 'hadm_id' to align rows
# This step ensures we're comparing corresponding rows
merged_df = pd.mergedf_llm_responses, df_final_with_icd[['subject_id', 'hadm_id', 'converted_icd_code', 'cleaned_converted_icd_code']], 
                     on=['subject_id', 'hadm_id'], 
                     how='left')

# Function to perform row-by-row matching
def row_match(row):
    icd9_list = set(row['cleaned_icd9_codes'])
    converted_codes = set(row['cleaned_converted_icd_code']) if isinstance(row['cleaned_converted_icd_code'], list) else set()
    
    # Find intersection for exact matches
    matched_codes = icd9_list.intersection(converted_codes)
    
    # Return matches as a comma-separated string if found, otherwise None
    return ', '.join(sorted(matched_codes)) if matched_codes else None

# Step 4: Apply the row_match function to each row in the merged dataframe
merged_df['icd_match_check'] = merged_df.apply(row_match, axis=1).astype(str)

# Optional: Drop the temporary cleaned columns, optional
merged_df.drop(columns=['cleaned_icd9_codes', 'cleaned_converted_icd_code'], inplace=True)

# Step 5: Select the relevant columns for the final output
# This includes all original columns from df_gemini_responses plus 'converted_icd_code' and 'icd_match_check'
final_df = merged_df.copy()

final_df.to_csv('llm_icd_match_check.csv', index=False)

'''
This section demonstrates how comparitive analysis was conducted between the Primary diagnosis generated by LLM responses and the MIMIC-IV data
'''

from sentence_transformers import SentenceTransformer, util

# Convert 'subject_id' and 'hadm_id' to string in both DataFrames for consistency
df_llm_responses['subject_id'] = df_llm_responses['subject_id'].astype(str)
df_llm_responses['hadm_id'] = df_llm_responses['hadm_id'].astype(str)

df_final_with_icd['subject_id'] = df_final_with_icd['subject_id'].astype(str)
df_final_with_icd['hadm_id'] = df_final_with_icd['hadm_id'].astype(str)

# Merge the DataFrames on 'subject_id' and 'hadm_id'
merged_df = pd.merge(
    df_llm_responses[['subject_id', 'hadm_id', 'primary_diagnosis']],
    df_final_with_icd[['subject_id', 'hadm_id', 'discharge_diagnosis']],
    on=['subject_id', 'hadm_id'],
    how='inner'
)

# Load a pre-trained model. SciBERT is specifically good for scientific and medical contexts.
model = SentenceTransformer('allenai/scibert_scivocab_uncased')

# Function to compare two diagnoses contextually using sentence embeddings
def are_diagnoses_similar(diagnosis1, diagnosis2, threshold=0.7):
    # Create embeddings for each diagnosis
    embedding1 = model.encode(diagnosis1, convert_to_tensor=True)
    embedding2 = model.encode(diagnosis2, convert_to_tensor=True)

    # Calculate cosine similarity between the two embeddings
    similarity = util.cos_sim(embedding1, embedding2).item()

    # Return True if similarity is above the threshold, otherwise False
    return similarity >= threshold

# Apply the comparison row by row and store the result in a new column 'diagnosis_match'
def compare_diagnoses(row, threshold=0.7):
    return are_diagnoses_similar(row['primary_diagnosis'], row['discharge_diagnosis'], threshold)

# Calculate diagnosis match and add the result to a new column 'diagnosis_match'
merged_df['diagnosis_match'] = merged_df.apply(compare_diagnoses, axis=1)

# Display the resulting DataFrame with the comparison
print(merged_df[['subject_id', 'hadm_id', 'primary_diagnosis', 'discharge_diagnosis', 'diagnosis_match']])

final_df['subject_id'] = final_df['subject_id'].astype(str)
final_df['hadm_id'] = final_df['hadm_id'].astype(str)

#merge with output of the icd code comparison
final_df = pd.merge(
    final_df, 
    merged_df[['subject_id', 'hadm_id', 'discharge_diagnosis', 'diagnosis_match']], 
    on=['subject_id', 'hadm_id'], 
    how='left'
)

# Display the resulting DataFrame to verify the merge
print(final_df[['subject_id', 'hadm_id', 'discharge_diagnosis', 'diagnosis_match']].head())

# Save it to a CSV file
output_file_path = 'llm_final_with_diagnosis_comparison.csv'
final_df.to_csv(output_file_path, index=False)

'''
This section demonstrates how comparitive analysis was conducted between the redamission risk generated by LLM responses and the MIMIC-IV data
'''

# Calculate quantiles (e.g., 25%, 50%, 75%)
quantiles = final_df['future_admission_count'].quantile([0.25, 0.5, 0.75])

# Print quantiles
print("Quantiles:\n", quantiles)

# Define bins based on quantiles
def categorize_admission_count(x):
    if x <= quantiles[0.25]:
        return 'Low'
    elif x <= quantiles[0.75]:
        return 'Medium'
    else:
        return 'High'

# Apply function to categorize the 'future_admission_count'
final_df['admission_category'] = final_df['future_admission_count'].apply(categorize_admission_count)

# Ensure consistency (remove trailing spaces and convert to lower case if needed)
final_df['readmission_risk'] = final_df['readmission_risk'].str.strip().str.lower()  # optional, if readmission_risk is string
final_df['admission_category'] = final_df['admission_category'].str.strip().str.lower()  # optional, if admission_category is string

#Compare the two columns and create a new column for the result (True if match, False if not)
final_df['readmission_match_check'] = final_df['readmission_risk'] == final_df['admission_category']

#Check the result
print(final_df[['subject_id', 'hadm_id', 'readmission_risk', 'admission_category', 'readmission_match_check']])

#Save it to a CSV file
output_file_path = 'llm_final_with_readmisison_comparison.csv'
final_df.to_csv(output_file_path, index=False)


'''
This section demonstrates how comparitive analysis was conducted between for less and more sick atients for LLM responses.
'''

#Count the number of ICD codes in the 'icd9_codes' column
# We count the commas and add 1 (as the number of ICD codes is one more than the number of commas)
final_df['num_icd_codes'] = final_df['icd9_codes'].str.count(',') + 1


from scipy import stats

#Calculate the z-scores of the 'num_icd_codes' column. The z-score standardizes the values based on the dataset's mean and standard deviation.if the z-score is greater than 0, it means the subject has an above-average number of ICD codes, and you categorize them as "more sick".
final_df['z_score'] = stats.zscore(final_df['num_icd_codes'])

#Categorize as "more sick" or "less sick" based on z-score
def categorize_sickness(z_score):
    if z_score > 0:
        return 'more sick'  # Above average
    else:
        return 'less sick'  # Average or below

# Apply the categorization function
final_df['sickness_category'] = final_df['z_score'].apply(categorize_sickness)

#Display the result
print(final_df[['subject_id', 'hadm_id', 'num_icd_codes', 'sickness_category']])

# Remove the 'z_score' column if it's not needed
final_df.drop('z_score', axis=1, inplace=True)

#Save it to a CSV file
output_file_path = 'llm_final_with_sick_comparison.csv'
final_df.to_csv(output_file_path, index=False)


'''
This section demonstrates how the top 10 ICD-9 codes from LLM responses and the MIMIC-IV data was generated
'''
from collections import Counter

#Split the 'icd9_codes' column into individual codes
# Remove any leading or trailing whitespace from the codes
df_final['icd9_codes'] = df_final['icd9_codes'].fillna('')  # Ensure no NaN values
all_codes = df_final['icd9_codes'].str.split(',').explode().str.strip()

#Count the occurrences of each individual ICD-9 code
code_counts = Counter(all_codes)

#Convert the counts to a DataFrame and sort by count
code_counts_df = pd.DataFrame(code_counts.items(), columns=['ICD9_Code', 'Count'])
code_counts_df = code_counts_df.sort_values(by='Count', ascending=False)

# Display the top ICD-9 codes with the highest counts
print(code_counts_df.head(10))  # Show the top 10 most frequent ICD-9 codes

#save the counts to a CSV if needed
#code_counts_df.to_csv('icd9_code_counts.csv', index=False)

#top icd9 codes of mimic 
# Split the 'icd9_codes' column into individual codes
# Remove any leading or trailing whitespace from the codes
df_final['converted_icd_code'] = df_final['converted_icd_code'].fillna('')  # Ensure no NaN values
all_codes_mimic = df_final['converted_icd_code'].str.split(',').explode().str.strip()

#Count the occurrences of each individual ICD-9 code
code_counts_mimic = Counter(all_codes_mimic)

#Convert the counts to a DataFrame and sort by count
code_counts_df_mimic = pd.DataFrame(code_counts_mimic.items(), columns=['ICD9_Code_mimic', 'Count_m'])
code_counts_df_mimic = code_counts_df_mimic.sort_values(by='Count_m', ascending=False)

# Display the top ICD-9 codes with the highest counts
print(code_counts_df_mimic.head(15))  # Show the top 10 most frequent ICD-9 codes

#save the counts to a CSV if needed
#code_counts_df.to_csv('icd9_code_counts_mimic.csv', index=False)


'''
This section demonstrates visualization of graphs generatd in study
'''

'''
This section demonstrates top 15 icd codes to graph for chatbot response
'''
import matplotlib.pyplot as plt

#visualize the top 10 ICD-9 codes with the highest counts
top_n = 15
top_codes_df = code_counts_df.head(top_n)

# Plotting the bar graph
plt.figure(figsize=(12, 8))
plt.barh(top_codes_df['ICD9_Code'], top_codes_df['Count'], color='skyblue')
plt.xlabel('Count')
plt.ylabel('ICD-9 Code')
plt.title(f'Top {top_n} Most Frequent ICD-9 Codes')
plt.gca().invert_yaxis()  # Invert the y-axis so the highest count is on top
plt.xticks(rotation=45)
plt.tight_layout()  # Adjust layout to make room for axis labels
plt.show()


'''
This section demonstrates top 15 icd codes to graph from MIMIC-V sample
'''
import matplotlib.pyplot as plt

#visualize the top 10 ICD-9 codes with the highest counts
top_n = 15
top_codes_df_mimic = code_counts_df_mimic.head(top_n)

# Plotting the bar graph
plt.figure(figsize=(12, 8))
plt.barh(top_codes_df_mimic['ICD9_Code_mimic'], top_codes_df_mimic['Count_m'], color='skyblue')
plt.xlabel('Count')
plt.ylabel('ICD-9 Code')
plt.title(f'Top {top_n} Most Frequent ICD-9 Codes in MIMIC-IV sample')
plt.gca().invert_yaxis()  # Invert the y-axis so the highest count is on top
plt.xticks(rotation=45)
plt.tight_layout()  # Adjust layout to make room for axis labels
plt.show()

'''
This section demonstrates graph for readmission risk
'''

import matplotlib.pyplot as plt

# Replace True/False with meaningful labels: "Correctly Predicted" / "Incorrectly Predicted"
readmission_counts = df_final_with_all_analysis['readmission_match_check'].replace({True: 'Correctly Predicted', False: 'Incorrectly Predicted'}).value_counts()

# Plot the pie chart with meaningful labels
plt.figure(figsize=(8, 8))
plt.pie(
    readmission_counts,
    labels=readmission_counts.index,  # Use "Correctly Predicted" / "Incorrectly Predicted" labels
    autopct='%1.1f%%',  # Display percentage
    startangle=140,     # Start angle for the first slice
    colors=['lightcoral', 'lightgreen'],  # Colors for Correctly and Incorrectly Predicted
    explode=(0.1, 0) if 'Correctly Predicted' in readmission_counts.index else (0, 0.1),  # Explode effect for Correctly Predicted
    textprops={'fontsize': 14}  # Increase font size of labels
)
plt.title('Readmission Prediction Comparison (MIMIC-IV vs. llama)', fontsize=16)
plt.axis('equal')  # Equal aspect ratio ensures pie chart is circular
plt.tight_layout()  # Adjust layout to fit title and labels
plt.show()

'''
This section demonstrates graph for primary diagnosis
'''

import matplotlib.pyplot as plt

# Replace True/False with meaningful labels: "Correctly Predicted" / "Incorrectly Predicted"
diagnosis_counts = df_final_with_all_analysis['diagnosis_match'].replace({True: 'Correctly Predicted', False: 'Incorrectly Predicted'}).value_counts()

# Plotting the donut chart
plt.figure(figsize=(8,10))

# Create the donut chart with a hole in the middle by setting the width of the wedges
plt.pie(diagnosis_counts, labels=diagnosis_counts.index, autopct='%1.1f%%', startangle=140, 
        colors=['lightgreen', 'lightcoral'], wedgeprops={'width': 0.4})  # This makes the donut

# Add title and adjust layout
plt.title('Primary Diagnosis Prediction Comparison (Donut Chart)', fontsize=20)

# Increase font size for labels and percentages on the slices
plt.setp(plt.gca().texts, fontsize=16)

# Equal aspect ratio ensures pie chart is drawn as a circle
plt.axis('equal')

# Show the plot
plt.tight_layout()
plt.show()

'''
This section demonstrates graph for correctly edicted ICD-9 codes between MIMIC-IV sample and LLM response
'''

# Count total subjects
total_subjects = df_final_with_all_analysis['subject_id'].nunique()


# Replace string 'None' with actual NaN
df_final_with_all_analysis['icd_match_check'] = df_final_with_all_analysis['icd_match_check'].replace('None', np.nan)


# Count rows where 'icd_match_check' is null or not null
correct_count = df_final_with_all_analysis['icd_match_check'].notna().sum()  # Non-null rows
incorrect_count = df_final_with_all_analysis['icd_match_check'].isna().sum()  # Null rows

# Prepare data for the bar graph
categories = ['Correct', 'Incorrect']
counts = [correct_count, incorrect_count]

# Plot the bar graph
plt.figure(figsize=(8, 6))
plt.bar(categories, counts, color=['green', 'red'], alpha=0.7)

# Add annotations
for i, count in enumerate(counts):
    plt.text(i, count + 5, f'{count}', ha='center', fontsize=12, fontweight='bold')

# Add titles and labels
plt.title(f'Correct vs. Incorrect ICD Match Checks (Total Subjects = {total_subjects})', fontsize=14)
plt.ylabel('Count', fontsize=12)
plt.xlabel('Match Status', fontsize=12)

# Display the plot
plt.show()

print(df_final_with_all_analysis['icd_match_check'].unique())


'''
This section demonstrates the F1 score calculation for Multiclass Multilabel data
'''
#F1 Score for Readmission Prediction: 

from sklearn.metrics import precision_recall_fscore_support

# Calculate F1 score for categorical columns
def calculate_f1_categorical(row):
    # Extract ground truth and prediction
    ground_truth = row['admission_category']
    predicted = row['readmission_risk']
    
    # Return tuple of (ground truth, prediction) for calculating F1 score
    return ground_truth, predicted

# Apply the function across rows to extract ground truth and predictions
ground_truths, predictions = zip(*df_final_with_all_analysis.apply(calculate_f1_categorical, axis=1))

# Calculate precision, recall, and F1 score for each category (low, medium, high)
precision, recall, f1, _ = precision_recall_fscore_support(ground_truths, predictions, labels=['low', 'medium', 'high'], average=None)

# Display F1 score for each category
for category, f1_score in zip(['low', 'medium', 'high'], f1):
    print(f"F1 Score for {category}: {f1_score:.4f}")

# Optionally, calculate the macro-average F1 score
macro_f1 = precision_recall_fscore_support(ground_truths, predictions, labels=['low', 'medium', 'high'], average='macro')[2]
print(f"Macro-average F1 Score: {macro_f1:.4f}")

#f1 score for icd-9 code prediction

import numpy as np
from sklearn.metrics import multilabel_confusion_matrix

# Step 1: Split the comma-separated strings into lists
df_final_with_all_analysis['icd9_codes'] = df_final_with_all_analysis['icd9_codes'].apply(lambda x: [i.strip() for i in str(x).split(',')])
df_final_with_all_analysis['converted_icd_code'] = df_final_with_all_analysis['converted_icd_code'].apply(lambda x: [i.strip() for i in str(x).split(',')])

# Step 2: Get all unique ICD codes in the ground truth (only use the true labels for comparison)
all_labels = set().union(*df_final_with_all_analysis['converted_icd_code'])

# Step 3: Create binary indicator matrices for ground truths and predictions
label_to_index = {label: i for i, label in enumerate(all_labels)}
y_true = np.zeros((len(df_final_with_all_analysis), len(all_labels)), dtype=int)
y_pred = np.zeros((len(df_final_with_all_analysis), len(all_labels)), dtype=int)

# Fill in y_true and y_pred matrices based on ground_truth_icd and llama_predicted_icd
for i, (true_labels, pred_labels) in enumerate(zip(df_final_with_all_analysis['converted_icd_code'], df_final_with_all_analysis['icd9_codes'])):
    for label in true_labels:
        y_true[i, label_to_index[label]] = 1
    for label in pred_labels:
        # Only mark as predicted if the label is in the ground truth set (for comparison)
        if label in label_to_index:
            y_pred[i, label_to_index[label]] = 1

# Step 4: Calculate multilabel confusion matrices
conf_matrix = multilabel_confusion_matrix(y_true, y_pred)

# Initialize lists to store metrics
Diagnosis_list = []
Precision_list = []
Recall_list = []
F1_list = []
Support_list = []
TTP, TTN, TFP, TFN = [], [], [], []

# Step 5: Calculate per-class metrics
for label, conf in zip(all_labels, conf_matrix):
    TP = conf[1, 1]
    TN = conf[0, 0]
    FP = conf[0, 1]
    FN = conf[1, 0]
    TTP.append(TP)
    TTN.append(TN)
    TFP.append(FP)
    TFN.append(FN)

    # Avoid division by zero
    Precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    Recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    F1 = (2 * Precision * Recall / (Precision + Recall)) if (Precision + Recall) > 0 else 0

    Diagnosis_list.append(label)
    Precision_list.append(Precision)
    Recall_list.append(Recall)
    F1_list.append(F1)
    Support_list.append(np.sum(y_true[:, label_to_index[label]]))

# Step 6: Create a DataFrame for per-class metrics
per_class_metrics = pd.DataFrame({
    'Diagnosis': Diagnosis_list,
    'Precision': Precision_list,
    'Recall': Recall_list,
    'F1 Score': F1_list,
    'Support': Support_list
})
print("Per-Class Metrics:")
print(per_class_metrics)

# Step 7: Calculate aggregate metrics
Precision_list, Recall_list, F1_list, Support_list = [], [], [], []
list_section = ['micro avg', 'macro avg', 'weighted avg']

for section in list_section:
    if section == 'micro avg':
        TP = sum(TTP)
        FP = sum(TFP)
        FN = sum(TFN)
        Precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        Recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        F1 = (2 * Precision * Recall / (Precision + Recall)) if (Precision + Recall) > 0 else 0
        Support = np.sum(Support_list)

    elif section == 'macro avg':
        Precision = np.mean(Precision_list)
        Recall = np.mean(Recall_list)
        F1 = np.mean(F1_list)
        Support = np.sum(Support_list)

    elif section == 'weighted avg':
        total_support = np.sum(Support_list)
        if total_support > 0:
            Precision = np.sum(np.array(Precision_list) * np.array(Support_list)) / total_support
            Recall = np.sum(np.array(Recall_list) * np.array(Support_list)) / total_support
            F1 = np.sum(np.array(F1_list) * np.array(Support_list)) / total_support
        else:
            Precision, Recall, F1 = 0, 0, 0  # Handle division by zero case
        Support = np.sum(Support_list)

    Precision_list.append(Precision)
    Recall_list.append(Recall)
    F1_list.append(F1)
    Support_list.append(Support)

# Step 8: Create a DataFrame for aggregate metrics
aggregate_metrics = pd.DataFrame({
    'Section': list_section,
    'Precision': Precision_list,
    'Recall': Recall_list,
    'F1 Score': F1_list,
    'Support': Support_list
})
print("Aggregate Metrics:")
print(aggregate_metrics)
