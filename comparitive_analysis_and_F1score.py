# -*- coding: utf-8 -*-
"""
Created on Mon Nov 25 20:44:38 2024

@author: Parvati, Raajitha
"""
import pandas as pd
import re
import numpy as np

'''
This py script will show how the comparitive analysis and visualization was conducted for the study.

The same script can be applied on each LLMs response that has been extracted into csv.

The immediate below section demonstrates how ICD9 Match analysis was conducted between the LLM responses and the MIMIC-IV data
'''
#loading our file with all mimic data
df_final_with_icd = pd.read_csv('final_sample_300_with_icd_conversions.csv')

#loading the csv with LLM response into their respective columns. as example here we load the llama. we later also load the other two LLM responses here.
df_llm_responses =  pd.read_csv('llama_response_sectioned.csv')

# set the datatypes
df_llm_responses['icd9_codes'] = df_llm_responses['icd9_codes'].astype(str)
df_final_with_icd['converted_icd_code'] = df_final_with_icd['converted_icd_code'].astype(str)

df_llm_responses['subject_id'] = df_llm_responses['subject_id'].astype(str)
df_final_with_icd['subject_id'] = df_final_with_icd['subject_id'].astype(str)

# Function to clean ICD codes by removing periods and trailing spaces as MIMIC-IV data did not have periods
def clean_icd_codes(codes):
    if pd.isna(codes):
        return []
    # Remove periods, strip spaces, and split by comma
    return [code.strip().replace('.', '') for code in codes.split(',')]

#Clean 'icd9_codes' in df_gemini_responses
df_llm_responses['cleaned_icd9_codes'] = df_llm_responses['icd9_codes'].apply(clean_icd_codes)

#Clean 'converted_icd_code' in df_final_with_icd
df_final_with_icd['cleaned_converted_icd_code'] = df_final_with_icd['converted_icd_code'].apply(
    lambda codes: [code.strip() for code in codes.split(',')]
)

#Merge dataframes on 'subject_id' and 'hadm_id' to align rows
# This step ensures we're comparing corresponding rows
merged_df = pd.mergedf_llm_responses, df_final_with_icd[['subject_id', 'hadm_id', 'converted_icd_code', 'cleaned_converted_icd_code']], 
                     on=['subject_id', 'hadm_id'], 
                     how='left')

# Function to perform row-by-row matching
def row_match(row):
    icd9_list = set(row['cleaned_icd9_codes'])
    converted_codes = set(row['cleaned_converted_icd_code']) if isinstance(row['cleaned_converted_icd_code'], list) else set()
    
    # Find intersection for exact matches
    matched_codes = icd9_list.intersection(converted_codes)
    
    # Return matches as a comma-separated string if found, otherwise None
    return ', '.join(sorted(matched_codes)) if matched_codes else None

# Step 4: Apply the row_match function to each row in the merged dataframe
merged_df['icd_match_check'] = merged_df.apply(row_match, axis=1).astype(str)

# Optional: Drop the temporary cleaned columns, optional
merged_df.drop(columns=['cleaned_icd9_codes', 'cleaned_converted_icd_code'], inplace=True)

# Step 5: Select the relevant columns for the final output
# This includes all original columns from df_gemini_responses plus 'converted_icd_code' and 'icd_match_check'
final_df = merged_df.copy()

final_df.to_csv('llm_icd_match_check.csv', index=False)

'''
This section demonstrates how comparitive analysis was conducted between the Primary diagnosis generated by LLM responses and the MIMIC-IV data
'''

from sentence_transformers import SentenceTransformer, util

# Convert 'subject_id' and 'hadm_id' to string in both DataFrames for consistency
df_llm_responses['subject_id'] = df_llm_responses['subject_id'].astype(str)
df_llm_responses['hadm_id'] = df_llm_responses['hadm_id'].astype(str)

df_final_with_icd['subject_id'] = df_final_with_icd['subject_id'].astype(str)
df_final_with_icd['hadm_id'] = df_final_with_icd['hadm_id'].astype(str)

# Merge the DataFrames on 'subject_id' and 'hadm_id'
merged_df = pd.merge(
    df_llm_responses[['subject_id', 'hadm_id', 'primary_diagnosis']],
    df_final_with_icd[['subject_id', 'hadm_id', 'discharge_diagnosis']],
    on=['subject_id', 'hadm_id'],
    how='inner'
)

# Load a pre-trained model. SciBERT is specifically good for scientific and medical contexts.
model = SentenceTransformer('allenai/scibert_scivocab_uncased')

# Function to compare two diagnoses contextually using sentence embeddings
def are_diagnoses_similar(diagnosis1, diagnosis2, threshold=0.7):
    # Create embeddings for each diagnosis
    embedding1 = model.encode(diagnosis1, convert_to_tensor=True)
    embedding2 = model.encode(diagnosis2, convert_to_tensor=True)

    # Calculate cosine similarity between the two embeddings
    similarity = util.cos_sim(embedding1, embedding2).item()

    # Return True if similarity is above the threshold, otherwise False
    return similarity >= threshold

# Apply the comparison row by row and store the result in a new column 'diagnosis_match'
def compare_diagnoses(row, threshold=0.7):
    return are_diagnoses_similar(row['primary_diagnosis'], row['discharge_diagnosis'], threshold)

# Calculate diagnosis match and add the result to a new column 'diagnosis_match'
merged_df['diagnosis_match'] = merged_df.apply(compare_diagnoses, axis=1)

# Display the resulting DataFrame with the comparison
print(merged_df[['subject_id', 'hadm_id', 'primary_diagnosis', 'discharge_diagnosis', 'diagnosis_match']])

final_df['subject_id'] = final_df['subject_id'].astype(str)
final_df['hadm_id'] = final_df['hadm_id'].astype(str)

#merge with output of the icd code comparison
final_df = pd.merge(
    final_df, 
    merged_df[['subject_id', 'hadm_id', 'discharge_diagnosis', 'diagnosis_match']], 
    on=['subject_id', 'hadm_id'], 
    how='left'
)

# Display the resulting DataFrame to verify the merge
print(final_df[['subject_id', 'hadm_id', 'discharge_diagnosis', 'diagnosis_match']].head())

# Save it to a CSV file
output_file_path = 'llm_final_with_diagnosis_comparison.csv'
final_df.to_csv(output_file_path, index=False)

'''
This section demonstrates how comparitive analysis was conducted between the redamission risk generated by LLM responses and the MIMIC-IV data
'''

# Calculate quantiles (e.g., 25%, 50%, 75%)
quantiles = final_df['future_admission_count'].quantile([0.25, 0.5, 0.75])

# Print quantiles
print("Quantiles:\n", quantiles)

# Define bins based on quantiles
def categorize_admission_count(x):
    if x <= quantiles[0.25]:
        return 'Low'
    elif x <= quantiles[0.75]:
        return 'Medium'
    else:
        return 'High'

# Apply function to categorize the 'future_admission_count'
final_df['admission_category'] = final_df['future_admission_count'].apply(categorize_admission_count)

# Ensure consistency (remove trailing spaces and convert to lower case if needed)
final_df['readmission_risk'] = final_df['readmission_risk'].str.strip().str.lower()  # optional, if readmission_risk is string
final_df['admission_category'] = final_df['admission_category'].str.strip().str.lower()  # optional, if admission_category is string

#Compare the two columns and create a new column for the result (True if match, False if not)
final_df['readmission_match_check'] = final_df['readmission_risk'] == final_df['admission_category']

#Check the result
print(final_df[['subject_id', 'hadm_id', 'readmission_risk', 'admission_category', 'readmission_match_check']])

#Save it to a CSV file
output_file_path = 'llm_final_with_readmisison_comparison.csv'
final_df.to_csv(output_file_path, index=False)


'''
This section demonstrates how comparitive analysis was conducted between for less and more sick atients for LLM responses.
'''

from scipy import stats

#Calculate the z-scores of the 'num_icd_codes' column. The z-score standardizes the values based on the dataset's mean and standard deviation.if the z-score is greater than 0, it means the subject has an above-average number of ICD codes, and you categorize them as "more sick".
final_df['z_score'] = stats.zscore(final_df['num_icd_codes'])

#Categorize as "more sick" or "less sick" based on z-score
def categorize_sickness(z_score):
    if z_score > 0:
        return 'more sick'  # Above average
    else:
        return 'less sick'  # Average or below

# Apply the categorization function
final_df['sickness_category'] = final_df['z_score'].apply(categorize_sickness)

#Display the result
print(final_df[['subject_id', 'hadm_id', 'num_icd_codes', 'sickness_category']])

# Remove the 'z_score' column if it's not needed
final_df.drop('z_score', axis=1, inplace=True)

#Save it to a CSV file
output_file_path = 'llm_final_with_sick_comparison.csv'
final_df.to_csv(output_file_path, index=False)


'''
This section demonstrates the F1 score calculation for Multiclass Multilabel data- Readmission risk and ICD-9 prediction
'''
#F1 Score for Readmission Prediction, repeat the same for each chatbot response: 

from sklearn.metrics import precision_recall_fscore_support

# Calculate F1 score for categorical columns
def calculate_f1_categorical(row):
    # Extract ground truth and prediction
    ground_truth = row['admission_category']
    predicted = row['readmission_risk']
    
    # Return tuple of (ground truth, prediction) for calculating F1 score
    return ground_truth, predicted

# Apply the function across rows to extract ground truth and predictions
ground_truths, predictions = zip(*df_final_with_all_analysis.apply(calculate_f1_categorical, axis=1))

# Calculate precision, recall, and F1 score for each category (low, medium, high)
precision, recall, f1, _ = precision_recall_fscore_support(ground_truths, predictions, labels=['low', 'medium', 'high'], average=None)

# Display F1 score for each category
for category, f1_score in zip(['low', 'medium', 'high'], f1):
    print(f"F1 Score for {category}: {f1_score:.4f}")

# Optionally, calculate the macro-average F1 score
macro_f1 = precision_recall_fscore_support(ground_truths, predictions, labels=['low', 'medium', 'high'], average='macro')[2]
print(f"Macro-average F1 Score: {macro_f1:.4f}")

#****************f1 score for icd-9 code prediction, repeat the same for each chatbot response*************

import numpy as np
from sklearn.metrics import multilabel_confusion_matrix

# Step 1: Split the comma-separated strings into lists
df_final_with_all_analysis['icd9_codes'] = df_final_with_all_analysis['icd9_codes'].apply(lambda x: [i.strip() for i in str(x).split(',')])
df_final_with_all_analysis['converted_icd_code'] = df_final_with_all_analysis['converted_icd_code'].apply(lambda x: [i.strip() for i in str(x).split(',')])

# Step 2: Get all unique ICD codes in the ground truth (only use the true labels for comparison)
all_labels = set().union(*df_final_with_all_analysis['converted_icd_code'])

# Step 3: Create binary indicator matrices for ground truths and predictions
label_to_index = {label: i for i, label in enumerate(all_labels)}
y_true = np.zeros((len(df_final_with_all_analysis), len(all_labels)), dtype=int)
y_pred = np.zeros((len(df_final_with_all_analysis), len(all_labels)), dtype=int)

# Fill in y_true and y_pred matrices based on ground_truth_icd and llama_predicted_icd
for i, (true_labels, pred_labels) in enumerate(zip(df_final_with_all_analysis['converted_icd_code'], df_final_with_all_analysis['icd9_codes'])):
    for label in true_labels:
        y_true[i, label_to_index[label]] = 1
    for label in pred_labels:
        # Only mark as predicted if the label is in the ground truth set (for comparison)
        if label in label_to_index:
            y_pred[i, label_to_index[label]] = 1

# Step 4: Calculate multilabel confusion matrices
conf_matrix = multilabel_confusion_matrix(y_true, y_pred)

# Initialize lists to store metrics
Diagnosis_list = []
Precision_list = []
Recall_list = []
F1_list = []
Support_list = []
TTP, TTN, TFP, TFN = [], [], [], []

# Step 5: Calculate per-class metrics
for label, conf in zip(all_labels, conf_matrix):
    TP = conf[1, 1]
    TN = conf[0, 0]
    FP = conf[0, 1]
    FN = conf[1, 0]
    TTP.append(TP)
    TTN.append(TN)
    TFP.append(FP)
    TFN.append(FN)

    # Avoid division by zero
    Precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    Recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    F1 = (2 * Precision * Recall / (Precision + Recall)) if (Precision + Recall) > 0 else 0

    Diagnosis_list.append(label)
    Precision_list.append(Precision)
    Recall_list.append(Recall)
    F1_list.append(F1)
    Support_list.append(np.sum(y_true[:, label_to_index[label]]))

# Step 6: Create a DataFrame for per-class metrics
per_class_metrics = pd.DataFrame({
    'Diagnosis': Diagnosis_list,
    'Precision': Precision_list,
    'Recall': Recall_list,
    'F1 Score': F1_list,
    'Support': Support_list
})
print("Per-Class Metrics:")
print(per_class_metrics)

# Step 7: Calculate aggregate metrics
Precision_list, Recall_list, F1_list, Support_list = [], [], [], []
list_section = ['micro avg', 'macro avg', 'weighted avg']

for section in list_section:
    if section == 'micro avg':
        TP = sum(TTP)
        FP = sum(TFP)
        FN = sum(TFN)
        Precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        Recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        F1 = (2 * Precision * Recall / (Precision + Recall)) if (Precision + Recall) > 0 else 0
        Support = np.sum(Support_list)

    elif section == 'macro avg':
        Precision = np.mean(Precision_list)
        Recall = np.mean(Recall_list)
        F1 = np.mean(F1_list)
        Support = np.sum(Support_list)

    elif section == 'weighted avg':
        total_support = np.sum(Support_list)
        if total_support > 0:
            Precision = np.sum(np.array(Precision_list) * np.array(Support_list)) / total_support
            Recall = np.sum(np.array(Recall_list) * np.array(Support_list)) / total_support
            F1 = np.sum(np.array(F1_list) * np.array(Support_list)) / total_support
        else:
            Precision, Recall, F1 = 0, 0, 0  # Handle division by zero case
        Support = np.sum(Support_list)

    Precision_list.append(Precision)
    Recall_list.append(Recall)
    F1_list.append(F1)
    Support_list.append(Support)

# Step 8: Create a DataFrame for aggregate metrics
aggregate_metrics = pd.DataFrame({
    'Section': list_section,
    'Precision': Precision_list,
    'Recall': Recall_list,
    'F1 Score': F1_list,
    'Support': Support_list
})
print("Aggregate Metrics:")
print(aggregate_metrics)
